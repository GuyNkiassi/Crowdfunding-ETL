# Crowdfunding-ETL Module 8 Challenge

Module 8 challenge concist of using ETL process to create data pipelines in Python, jupyter notebook and PostgreSQL


# Project Overview

we utilize Python script to perform all 3 ETL steps. we will also perform an explorarory data analysis to show data realtionships that exist between tables.

    The goals of this challenge are;

    1. Create an ETL pipeline
    2. Data extraction from diffrent sources
    3. Scrub, clean and transform data to meaningful status
    4. Load newly discover data into a repository



# Resources

. CSV files
. Databases
. Software 

# Challenge Summary

1. Data intorduce will always be in the correct format like .jason or .csv. if we imported any other forms of data it will break

2. The spelling of columns names will not changed. Columns will not be removed form datasets.

3. The formatting of dollars amounts, date, time, or other numbers willl not change as this could break our regex expressions

4. THe SQL data table is locked and will not be dropped. The data will always be deleted and replaced instead of appended to existing  data.

# Conclusion

A data pipeline moves data from a source to a destination, and the ETL process creates data pipeline that also transform data along the way. Analysis is practically impossible without a good data, so creating data pipeline is often the first step before any anlysis can be performed. Therefore, understandin ETL is an essential skill for data anlysis.



